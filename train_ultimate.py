import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast
from torch.optim.swa_utils import AveragedModel, SWALR, update_bn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import os
import random
import gc
from tqdm import tqdm

# =============================================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG)
# =============================================================================
FILE_DU_LIEU = 'du_lieu_chiem_tinh_chuan_gio.parquet'

# --- C·∫§U H√åNH TAM H·ª¢P (3 KHUNG TH·ªúI GIAN) ---
# ƒê·∫°i ca ch·∫°y 3 khung n√†y ƒë·ªÉ b·∫Øt ƒë·ªß lo·∫°i c·∫ßu (ng·∫Øn, trung, d√†i)
LIST_LOOKBACK = [7, 30, 90]

# M·ªói khung ch·∫°y 3 model (Seed) ƒë·ªÉ lo·∫°i b·ªè r·ªßi ro
NUM_SEEDS_PER_LB = 3

# T·ªïng s·ªë epoch (v√≤ng l·∫∑p) train cho m·ªói model
EPOCHS = 1000

# B·∫Øt ƒë·∫ßu ch·∫ø ƒë·ªô SWA (Gom bi) t·ª´ epoch n√†y
SWA_START_EPOCH = 800

# C√°c th√¥ng s·ªë c·ªë ƒë·ªãnh kh√°c
NUM_CLASSES = 100            # 100 s·ªë (00-99)
BATCH_SIZE = 1024
LEARNING_RATE = 0.001
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"‚öôÔ∏è KH·ªûI ƒê·ªòNG CHI·∫æN D·ªäCH TAM H·ª¢P (FULL EXPANDED) TR√äN: {DEVICE}")
if DEVICE.type == 'cuda':
    print(f"üöÄ GPU Detected: {torch.cuda.get_device_name(0)}")


# =============================================================================
# 2. C√ÅC L·ªöP H·ªñ TR·ª¢ (LOSS & LAYERS)
# =============================================================================

class FocalLossWithSmoothing(nn.Module):
    """
    H√†m Loss k·∫øt h·ª£p Focal Loss v√† Label Smoothing.
    - Focal Loss: Ph·∫°t n·∫∑ng model n·∫øu ƒëo√°n sai s·ªë tr√∫ng (s·ªë hi·∫øm).
    - Smoothing: Gi√∫p model kh√¥ng b·ªã qu√° t·ª± tin (Overconfidence).
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', smoothing=0.05):
        super(FocalLossWithSmoothing, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.smoothing = smoothing
        self.bce = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, inputs, targets):
        # L√†m m·ªÅm nh√£n (0 -> 0.05, 1 -> 0.95)
        targets_smooth = targets * (1 - self.smoothing) + 0.5 * self.smoothing
        
        # T√≠nh BCE Loss c∆° b·∫£n
        bce_loss = self.bce(inputs, targets_smooth)
        
        # T√≠nh x√°c su·∫•t (pt)
        pt = torch.exp(-bce_loss)
        
        # √Åp d·ª•ng c√¥ng th·ª©c Focal Loss
        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class GatedResidualNetwork(nn.Module):
    """
    M·∫°ng Gated Residual (GRN) - C√¥ng ngh·ªá l·ªçc nhi·ªÖu.
    """
    def __init__(self, input_size, hidden_size, dropout=0.3):
        super(GatedResidualNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.elu = nn.ELU()
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.dropout = nn.Dropout(dropout)
        self.gate = nn.Linear(input_size, hidden_size)
        self.norm = nn.LayerNorm(hidden_size)
        
        if input_size != hidden_size:
            self.project = nn.Linear(input_size, hidden_size)
        else:
            self.project = None

    def forward(self, x):
        # Nh√°nh t·∫Øt (Residual)
        residual = self.project(x) if self.project is not None else x
        
        # Nh√°nh ch√≠nh
        x_val = self.fc1(x)
        x_val = self.elu(x_val)
        x_val = self.fc2(x_val)
        x_val = self.dropout(x_val)
        
        # C·ªïng l·ªçc (Gate)
        gate_val = torch.sigmoid(self.gate(x))
        
        # K·∫øt h·ª£p
        out = (x_val * gate_val) + residual
        return self.norm(out)


# =============================================================================
# 3. KI·∫æN TR√öC M√î H√åNH (ULTIMATE ASTRO MODEL)
# =============================================================================

class UltimateAstroModel(nn.Module):
    def __init__(self, num_classes, lookback_days, astro_features, d_model=64, nhead=4):
        super(UltimateAstroModel, self).__init__()
        
        # --- A. NH√ÅNH L·ªäCH S·ª¨ ---
        self.hist_proj = nn.Linear(num_classes, d_model)
        # Positional Embedding (H·ªçc v·ªã tr√≠ th·ªùi gian)
        self.pos_embedding = nn.Parameter(torch.randn(1, lookback_days, d_model))
        # C·ªïng l·ªçc nhi·ªÖu l·ªãch s·ª≠
        self.hist_gate = GatedResidualNetwork(d_model, d_model)
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead, 
            batch_first=True, 
            dropout=0.1
        )
        self.hist_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)
        
        # --- B. NH√ÅNH CHI√äM TINH ---
        # C·ªïng l·ªçc nhi·ªÖu sao
        self.astro_gate = GatedResidualNetwork(astro_features, d_model)
        
        # --- C. CROSS ATTENTION ---
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=d_model, 
            num_heads=nhead, 
            batch_first=True
        )
        self.norm_cross = nn.LayerNorm(d_model)
        
        # --- D. FUSION ---
        self.fusion_gate = GatedResidualNetwork(d_model * 2, 128)
        self.final_out = nn.Linear(128, num_classes)

    def forward(self, x_hist, x_astro):
        # 1. X·ª≠ l√Ω L·ªãch s·ª≠
        h = self.hist_proj(x_hist) + self.pos_embedding
        h = self.hist_gate(h)
        h = self.hist_encoder(h) # Output: (Batch, Lookback, d_model)
        
        # 2. X·ª≠ l√Ω Chi√™m tinh
        a = self.astro_gate(x_astro)
        a_query = a.unsqueeze(1) # Output: (Batch, 1, d_model)
        
        # 3. Cross Attention (D√πng Sao soi L·ªãch s·ª≠)
        attn_out, _ = self.cross_attention(query=a_query, key=h, value=h)
        attn_out = self.norm_cross(attn_out + a_query)
        
        # 4. H·ª£p nh·∫•t
        a_squeezed = a_query.squeeze(1)
        attn_squeezed = attn_out.squeeze(1)
        combined = torch.cat((a_squeezed, attn_squeezed), dim=1)
        
        # 5. Output
        return self.final_out(self.fusion_gate(combined))


# =============================================================================
# 4. DATASET V√Ä H√ÄM LOAD D·ªÆ LI·ªÜU
# =============================================================================

class LotteryDataset(Dataset):
    def __init__(self, x_hist, x_astro, y):
        self.x_hist = torch.tensor(x_hist, dtype=torch.float32)
        self.x_astro = torch.tensor(x_astro, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.x_hist[idx], self.x_astro[idx], self.y[idx]


def load_data_dynamic(lookback_days):
    """
    H√†m load d·ªØ li·ªáu ƒë·ªông d·ª±a theo tham s·ªë lookback_days.
    T·ª± ƒë·ªông x·ª≠ l√Ω ng√†y r·ªóng v√† chuy·ªÉn ƒë·ªïi vector.
    """
    if not os.path.exists(FILE_DU_LIEU):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {FILE_DU_LIEU}")
        return None
    
    print(f"üìÇ ƒêang x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Lookback = {lookback_days} ng√†y...")
    df = pd.read_parquet(FILE_DU_LIEU)
    
    # X√°c ƒë·ªãnh c·ªôt
    res_cols = [c for c in df.columns if c.startswith('Res_')]
    astro_cols = [c for c in df.columns if c not in res_cols and c != 'Date' and not c.endswith('_Deg')]
    
    total_days = len(df)
    daily_vectors = np.zeros((total_days, NUM_CLASSES), dtype='float32')
    raw_results = df[res_cols].values 
    
    # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ sang Vector 100 chi·ªÅu
    for i in range(total_days):
        for num in raw_results[i]:
            if pd.notna(num):
                try:
                    idx = int(num)
                    if 0 <= idx <= 99:
                        daily_vectors[i, idx] = 1.0
                except:
                    pass
    
    astro_data = df[astro_cols].values.astype('float32')
    
    X_h, X_a, Y = [], [], []
    
    # C·∫Øt c·ª≠a s·ªï tr∆∞·ª£t (Sliding Window)
    for i in range(lookback_days, len(daily_vectors)):
        target = daily_vectors[i]
        
        # B·ªé QUA NG√ÄY NGH·ªà (N·∫øu target to√†n s·ªë 0)
        if np.sum(target) == 0:
            continue
            
        # L·∫•y l·ªãch s·ª≠ (bao g·ªìm c·∫£ ng√†y ngh·ªâ ƒë·ªÉ gi·ªØ m·∫°ch th·ªùi gian)
        X_h.append(daily_vectors[i-lookback_days : i])
        X_a.append(astro_data[i])
        Y.append(target)
        
    return np.array(X_h), np.array(X_a), np.array(Y), len(astro_cols)


def seed_everything(seed=42):
    """Thi·∫øt l·∫≠p h·∫°t gi·ªëng ng·∫´u nhi√™n ƒë·ªÉ k·∫øt qu·∫£ t√°i l·∫≠p ƒë∆∞·ª£c"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True


# =============================================================================
# 5. C∆† CH·∫æ CHECKPOINT (L∆ØU GAME & CH∆†I TI·∫æP)
# =============================================================================
# ƒê√¢y l√† ph·∫ßn ƒë·∫°i ca c·∫ßn ki·ªÉm tra k·ªπ, em ƒë√£ vi·∫øt r·∫•t r√µ r√†ng

def save_checkpoint(model, optimizer, scheduler, swa_model, swa_scheduler, scaler, epoch, loss, filename):
    print(f"   üíæ [Checkpoint] ƒêang l∆∞u tr·∫°ng th√°i t·∫°i Epoch {epoch+1}...")
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'swa_model_state_dict': swa_model.state_dict() if swa_model else None,
        'swa_scheduler_state_dict': swa_scheduler.state_dict() if swa_scheduler else None,
        'scaler_state_dict': scaler.state_dict() if scaler else None,
        'loss': loss,
    }, filename)


def load_checkpoint(model, optimizer, scheduler, swa_model, swa_scheduler, scaler, filename):
    if os.path.isfile(filename):
        print(f"‚ôªÔ∏è  PH√ÅT HI·ªÜN CHECKPOINT '{filename}'. ƒêANG KH√îI PH·ª§C...")
        
        # --- S·ª¨A L·ªñI T·∫†I ƒê√ÇY (Th√™m weights_only=False) ---
        try:
            checkpoint = torch.load(filename, weights_only=False)
        except Exception as e:
            # D·ª± ph√≤ng cho c√°c phi√™n b·∫£n PyTorch c≈© h∆°n kh√¥ng c√≥ tham s·ªë n√†y
            checkpoint = torch.load(filename)
            
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        if swa_model and checkpoint.get('swa_model_state_dict'):
            swa_model.load_state_dict(checkpoint['swa_model_state_dict'])
        
        if swa_scheduler and checkpoint.get('swa_scheduler_state_dict'):
            swa_scheduler.load_state_dict(checkpoint['swa_scheduler_state_dict'])
            
        if scaler and checkpoint.get('scaler_state_dict'):
            scaler.load_state_dict(checkpoint['scaler_state_dict'])
            
        start_epoch = checkpoint['epoch'] + 1
        loss = checkpoint['loss']
        print(f"‚úÖ KH√îI PH·ª§C TH√ÄNH C√îNG! TI·∫æP T·ª§C TRAIN T·ª™ EPOCH {start_epoch}")
        return start_epoch, loss
    else:
        print("üÜï Kh√¥ng c√≥ checkpoint c≈©. B·∫Øt ƒë·∫ßu train m·ªõi.")
        return 0, None

# =============================================================================
# 6. H√ÄM TRAIN M·ªòT MODEL C·ª§ TH·ªÇ
# =============================================================================

def train_model(lookback, seed_idx, X_h, X_a, Y, num_astro_features):
    # Thi·∫øt l·∫≠p Seed ri√™ng bi·ªát
    seed = 42 + seed_idx
    seed_everything(seed)
    
    # ƒê·ªãnh danh c√°c file (QUAN TR·ªåNG ƒê·ªÇ KH√îNG B·ªä GHI ƒê√à)
    base_name = f"lb{lookback}_seed{seed}"
    model_name_swa = f"model_{base_name}_SWA.pth"   # File k·∫øt qu·∫£ cu·ªëi c√πng
    model_name_best = f"model_{base_name}_BEST.pth" # File k·∫øt qu·∫£ t·ªët nh·∫•t gi·ªØa ch·ª´ng
    checkpoint_name = f"checkpoint_{base_name}.pth" # File l∆∞u t·∫°m
    
    print(f"\n{'='*60}")
    print(f"‚ö° B·∫ÆT ƒê·∫¶U TRAIN: Lookback={lookback} | Seed={seed}")
    print(f"{'='*60}")
    
    # Chia d·ªØ li·ªáu 9/1
    X_h_train, X_h_test, X_a_train, X_a_test, Y_train, Y_test = train_test_split(
        X_h, X_a, Y, test_size=0.1, shuffle=False
    )
    
    train_loader = DataLoader(LotteryDataset(X_h_train, X_a_train, Y_train), batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(LotteryDataset(X_h_test, X_a_test, Y_test), batch_size=BATCH_SIZE, shuffle=False)
    
    # Kh·ªüi t·∫°o Model & Optimizer
    model = UltimateAstroModel(NUM_CLASSES, lookback, num_astro_features).to(DEVICE)
    swa_model = AveragedModel(model)
    
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=1e-5)
    swa_scheduler = SWALR(optimizer, swa_lr=0.05)
    
    criterion = FocalLossWithSmoothing(smoothing=0.05)
    scaler = GradScaler() if DEVICE.type == 'cuda' else None
    
    # --- KH√îI PH·ª§C CHECKPOINT N·∫æU C√ì ---
    start_epoch, _ = load_checkpoint(model, optimizer, scheduler, swa_model, swa_scheduler, scaler, checkpoint_name)
    
    best_val_loss = float('inf')

    # N·∫øu model n√†y ƒë√£ train xong tr∆∞·ªõc ƒë√≥ r·ªìi th√¨ b·ªè qua
    if start_epoch >= EPOCHS:
        print(f"‚è© Model {base_name} ƒë√£ ho√†n th√†nh tr∆∞·ªõc ƒë√≥. B·ªè qua.")
        return model_name_swa

    # --- V√íNG L·∫∂P TRAIN ---
    for epoch in range(start_epoch, EPOCHS):
        model.train()
        running_loss = 0.0
        
        # Ch·ªâ hi·ªÉn th·ªã progress bar cho 5 epoch ƒë·∫ßu v√† c√°c epoch chia h·∫øt cho 10
        show_progress = (epoch < 5) or ((epoch+1) % 10 == 0)
        
        if show_progress:
            loop = tqdm(train_loader, desc=f"Ep {epoch+1}/{EPOCHS}", leave=False)
        else:
            loop = train_loader

        for x_h, x_a, y in loop:
            x_h, x_a, y = x_h.to(DEVICE), x_a.to(DEVICE), y.to(DEVICE)
            optimizer.zero_grad()
            
            # Mixed Precision Training
            if scaler:
                with autocast():
                    outputs = model(x_h, x_a)
                    loss = criterion(outputs, y)
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                outputs = model(x_h, x_a)
                loss = criterion(outputs, y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
            running_loss += loss.item()
            if show_progress:
                loop.set_postfix(loss=loss.item())

        # C·∫≠p nh·∫≠t SWA ho·∫∑c Scheduler th∆∞·ªùng
        if epoch >= SWA_START_EPOCH:
            swa_model.update_parameters(model)
            swa_scheduler.step()
        else:
            scheduler.step()
            
        # --- VALIDATION & SAVE ---
        # Ki·ªÉm tra m·ªói 5 epoch
        if (epoch+1) % 5 == 0:
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for x_h, x_a, y in test_loader:
                    x_h, x_a, y = x_h.to(DEVICE), x_a.to(DEVICE), y.to(DEVICE)
                    out = model(x_h, x_a)
                    val_loss += criterion(out, y).item()
            
            avg_val_loss = val_loss / len(test_loader)
            avg_train_loss = running_loss / len(train_loader)
            
            msg = "SWA" if epoch >= SWA_START_EPOCH else "Normal"
            print(f"   [{msg}] Ep {epoch+1}: TrainLoss={avg_train_loss:.4f} | ValLoss={avg_val_loss:.4f}")
            
            # 1. L∆∞u Best Model (Model th∆∞·ªùng c√≥ Loss th·∫•p nh·∫•t)
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save(model.state_dict(), model_name_best)
                print(f"      üèÜ New Best Model Saved (Loss: {best_val_loss:.4f})")
            
            # 2. L∆∞u Checkpoint (ƒê·ªÉ l·ª° m·∫•t ƒëi·ªán th√¨ ch·∫°y l·∫°i)
            save_checkpoint(model, optimizer, scheduler, swa_model, swa_scheduler, scaler, epoch, avg_train_loss, checkpoint_name)

    # --- K·∫æT TH√öC V√íNG L·∫∂P ---
    print(f"üíæ ƒêang l∆∞u SWA Model ho√†n ch·ªânh: {model_name_swa}...")
    update_bn(train_loader, swa_model, device=DEVICE)
    torch.save(swa_model.state_dict(), model_name_swa)
    
    # X√≥a file checkpoint t·∫°m ƒëi cho s·∫°ch ·ªï c·ª©ng
    if os.path.exists(checkpoint_name):
        os.remove(checkpoint_name)
    
    # D·ªçn d·∫πp b·ªô nh·ªõ GPU
    del model, swa_model, optimizer, scaler
    torch.cuda.empty_cache()
    gc.collect()
    
    return model_name_swa


# =============================================================================
# 7. MAIN PROGRAM
# =============================================================================
def main():
    saved_files = []
    
    # V√≤ng l·∫∑p 1: Duy·ªát qua t·ª´ng khung th·ªùi gian (7, 30, 90)
    for lb in LIST_LOOKBACK:
        print(f"\n\n{'#'*60}")
        print(f"üåê CHUY·ªÇN SANG KHUNG TH·ªúI GIAN: LOOKBACK = {lb} NG√ÄY")
        print(f"{'#'*60}")
        
        # Load l·∫°i d·ªØ li·ªáu theo lookback m·ªõi
        data = load_data_dynamic(lb)
        if data is None: continue
        X_h, X_a, Y, num_features = data
        
        # V√≤ng l·∫∑p 2: Ch·∫°y nhi·ªÅu Seeds ƒë·ªÉ Ensemble
        for s in range(NUM_SEEDS_PER_LB):
            fname = train_model(lb, s, X_h, X_a, Y, num_features)
            saved_files.append(fname)
            
    print("\n" + "="*60)
    print("üéâ S·ª® M·ªÜNH HO√ÄN TH√ÄNH! ƒê·∫†I CA ƒê√É C√ì ƒê·ª¶ B·ªò S∆ØU T·∫¨P:")
    for f in saved_files:
        print(f"   ‚úÖ {f}")
    print("="*60)
    print("üëâ ƒê·∫°i ca gi·ªØ k·ªπ c√°c file n√†y ƒë·ªÉ d√πng cho code D·ª± B√°o nh√©!")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nüõë ƒê√£ d·ª´ng ch∆∞∆°ng tr√¨nh! Checkpoint an to√†n.")